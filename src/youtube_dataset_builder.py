import os
import re
from typing import List, Dict

from transformers import AutoTokenizer

from src.youtube_scraper import YouTubeScraper
from src.subtitle_preprocessor import SubtitlePreprocessor
from src.dataset_saver import DatasetSaver
from src.utils.logger_loader import LoggerLoader
from src.utils.config_model import AppConfig


class YouTubeDatasetBuilder:
    """
    –°–±–æ—Ä –¥–∞—Ç–∞—Å–µ—Ç–∞: —Å–∫–∞—á–∏–≤–∞–Ω–∏–µ, –æ—á–∏—Å—Ç–∫–∞, —Ä–∞–∑–±–∏–≤–∫–∞ –Ω–∞ —á–∞–Ω–∫–∏ –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ JSON.
    """

    CHUNK_SIZE: int = 500  # —á–∏—Å–ª–æ —Ç–æ–∫–µ–Ω–æ–≤ –≤ –æ–¥–Ω–æ–º —á–∞–Ω–∫–µ

    def __init__(self, cfg: AppConfig):
        self.cfg: AppConfig = cfg
        self.logger = LoggerLoader().get_logger()

        if not self.cfg:
            raise ValueError("‚ùå –ö–æ–Ω—Ñ–∏–≥ –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω")

        # –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
        self.subtitle_dir: str = cfg.subtitles_dir
        os.makedirs(self.subtitle_dir, exist_ok=True)
        self.scraper = YouTubeScraper(self.subtitle_dir)

        self.output_dir: str = cfg.output_dir
        os.makedirs(self.output_dir, exist_ok=True)
        self.saver = DatasetSaver(os.path.join(self.output_dir, "dataset.json"))

        # –¥–ª—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏ –ø—Ä–∏ —á–∞–Ω–∫–∏–Ω–≥–µ
        self.tokenizer = AutoTokenizer.from_pretrained(cfg.model_name)

        # —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        self.total_videos: int = 0
        self.downloaded_subtitles: int = 0
        self.skipped_videos: List[Dict[str, str]] = []

    def _chunk_text(self, text: str) -> List[str]:
        """
        –†–∞–∑–±–∏–≤–∞–µ—Ç –æ–¥–∏–Ω –¥–ª–∏–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç –Ω–∞ —Å–ø–∏—Å–æ–∫ —á–∞–Ω–∫–æ–≤ –ø–æ CHUNK_SIZE —Ç–æ–∫–µ–Ω–æ–≤,
        –¥–µ–∫–æ–¥–∏—Ä—É–µ—Ç –∏—Ö –∏ –Ω–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç –¥–≤–æ–π–Ω—ã–µ –¥–µ—Ñ–∏—Å—ã.
        """
        all_ids: List[int] = self.tokenizer.encode(text, add_special_tokens=False)
        chunks: List[str] = []
        for i in range(0, len(all_ids), self.CHUNK_SIZE):
            chunk_ids = all_ids[i : i + self.CHUNK_SIZE]
            # –¥–µ–∫–æ–¥–∏—Ä—É–µ–º –æ–±—Ä–∞—Ç–Ω–æ –≤ —Ç–µ–∫—Å—Ç
            chunk_text = self.tokenizer.decode(chunk_ids, clean_up_tokenization_spaces=True)
            # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –¥–≤–æ–π–Ω—ã–µ –¥–µ—Ñ–∏—Å—ã: " - - " ‚Üí "--"
            chunk_text = re.sub(r"\s*-\s*-\s*", "--", chunk_text.strip())
            chunks.append(chunk_text)
        return chunks

    def build_dataset(self) -> None:
        dataset: List[Dict[str, str]] = []

        for category, urls in self.cfg.categories.items():
            for url in urls:
                self.total_videos += 1
                self.logger.info(f"üîç –û–±—Ä–∞–±–∞—Ç—ã–≤–∞—é {url} (–∫–∞—Ç–µ–≥–æ—Ä–∏—è: {category})")
                try:
                    # 1) –°–∫–∞—á–∞—Ç—å —Å—É–±—Ç–∏—Ç—Ä—ã
                    path_vtt = self.scraper.download_subtitles(url)
                    if not path_vtt:
                        self.logger.warning(f"‚ö†Ô∏è –ù–µ—Ç —Å–∞–±–æ–≤: {url}")
                        self.skipped_videos.append({"url": url, "reason": "–ù–µ—Ç —Å–∞–±–æ–≤"})
                        continue
                    self.downloaded_subtitles += 1

                    # 2) –û—á–∏—Å—Ç–∫–∞
                    cleaned_txt = path_vtt.replace(".vtt", "_cleaned.txt")
                    SubtitlePreprocessor(path_vtt, cleaned_txt).process()

                    # 3) –ß—Ç–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞
                    try:
                        with open(cleaned_txt, "r", encoding="utf-8") as f:
                            text: str = f.read()
                    except Exception as e:
                        self.logger.error(f"–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è {cleaned_txt}: {e}")
                        self.skipped_videos.append({"url": url, "reason": "–ß—Ç–µ–Ω–∏–µ —Ñ–∞–π–ª–∞"})
                        continue

                    if not text.strip():
                        self.logger.warning(f"‚ö†Ô∏è –ü—É—Å—Ç–æ–π —Ç–µ–∫—Å—Ç: {url}")
                        self.skipped_videos.append({"url": url, "reason": "–ü—É—Å—Ç–æ–π —Ç–µ–∫—Å—Ç"})
                        continue

                    # 4) –†–∞–∑–±–∏–≤–∫–∞ –Ω–∞ —á–∞–Ω–∫–∏ –∏ –¥–æ–±–∞–≤–ª–µ–Ω–∏–µ –≤ –¥–∞—Ç–∞—Å–µ—Ç
                    for chunk in self._chunk_text(text):
                        dataset.append({
                            "category": category,
                            "text": chunk
                        })

                except Exception as e:
                    self.logger.exception(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ {url}: {e}")
                    self.skipped_videos.append({"url": url, "reason": "–û–±—â–∞—è –æ—à–∏–±–∫–∞"})
                    continue

        # 5) –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
        if dataset:
            try:
                self.saver.save(dataset)
                self.logger.info(f"‚úÖ –î–∞—Ç–∞—Å–µ—Ç —Å–æ—Ö—Ä–∞–Ω—ë–Ω –≤ {self.output_dir}")
            except Exception as e:
                self.logger.error(f"–û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è: {e}")
        else:
            self.logger.warning("‚ö†Ô∏è –î–∞—Ç–∞—Å–µ—Ç –ø—É—Å—Ç")

        # —Ñ–∏–Ω–∞–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        self.logger.info(f"üìä –ó–∞–≥—Ä—É–∂–µ–Ω–æ {self.downloaded_subtitles}/{self.total_videos} –≤–∏–¥–µ–æ")
        if self.skipped_videos:
            self.logger.warning("‚ö†Ô∏è –ü—Ä–æ–ø—É—â–µ–Ω–Ω—ã–µ –≤–∏–¥–µ–æ:")
            for it in self.skipped_videos:
                self.logger.warning(f"   ‚ùå {it['url']} ‚Äî {it['reason']}")

        print(f"\nüìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞: {self.downloaded_subtitles}/{self.total_videos} –≤–∏–¥–µ–æ")
        if self.skipped_videos:
            print("‚ö†Ô∏è –ü—Ä–æ–ø—É—â–µ–Ω–Ω—ã–µ –≤–∏–¥–µ–æ:")
            for it in self.skipped_videos:
                print(f"   ‚ùå {it['url']} ‚Äî {it['reason']}")
